# -*- coding: utf-8 -*-
"""Untitled11.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/142Ktddi9zJKm52SWNVFMRyaFwzUCChxK
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
import arviz as az
import pymc3 as pm
from theano import shared
from sklearn import preprocessing

pip install arviz

train = pd.read_csv("Training-Data-Sets - NEWDATA.csv")
train.head(2)

test = pd.read_csv("/content/Test dataset v1.xlsx - Sheet 1.csv")
test.head(5)

x_train = train.drop("EQ", axis=1)
y_train = train["EQ"]

x_train.head(2)

x_train.shape

x_test = test.drop("EQ", axis=1)
y_test = test["EQ"]
x_test.head(2)

mean=y_train.mean
mean

mean_df=pd.DataFrame()
Period = 0
start = 0
end = 28

while (end< len(x_train)):
  
  mean_temp=0
  temp= train[start:end]
  mean_temp=temp.mean(axis=0)
  mean_df=mean_df.append(mean_temp,ignore_index=True)
  
  start = start + 28
  end= end + 28

mean_df



train_data= mean_df.drop("EQ", axis=1)
train_y = mean_df["EQ"]

train_data.shape

correlation1=mean_df.corr()['EQ'].sort_values()
correlation1

correlation2=train.corr()['EQ'].sort_values()
correlation2

features = train_data[["EQ_Base_Price","RPI_Category","OOH_Working_Cost","EQ_Subcategory" , "pct_PromoMarketDollars_Subcategory" , "EQ_Category" , "Inflation" , "pct_PromoMarketDollars_Category" ,"Any_Promo_pct_ACV", "Social_Search_Impressions" ,"Median_Rainfall","Median_Temp","Magazine_Impressions_pct","OOH_Impressions","Competitor3_RPI","RPI_Subcategory" ]]
features.head(5)

test_set= x_test[["EQ_Base_Price","RPI_Category","OOH_Working_Cost","EQ_Subcategory" , "pct_PromoMarketDollars_Subcategory" , "EQ_Category" , "Inflation" , "pct_PromoMarketDollars_Category" ,"Any_Promo_pct_ACV", "Social_Search_Impressions" ,"Median_Rainfall","Median_Temp","Magazine_Impressions_pct","OOH_Impressions","Competitor3_RPI","RPI_Subcategory" ]]
test_set.shape

features.shape



data = train.sample(frac=0.01, random_state=99)
data.head(3)

data.isnull().sum()/ len(data)

az.plot_kde(data['EQ'].values, rug=True)
plt.yticks([0], alpha=0);

with pm.Model() as model_g:
    mu = pm.Normal('mu', mu=0, sd=5)
    tau = pm.HalfCauchy('tau', beta=5)
    theta_tilde = pm.Normal('theta_tilde',mu=0, sd=1)
    theta= pm.Deterministic('theta',mu+tau*theta_tilde)
    y = pm.Normal('y', mu=theta, sd=tau, observed=data['EQ'].values)
    trace_g = pm.sample(1000, chains=2)
    prior = pm.sample_prior_predictive()
    posterior_predictive = pm.sample_posterior_predictive(trace_g)

    pm.data = az.from_pymc3(
      trace=trace_g,
      prior=prior,
      posterior_predictive=posterior_predictive,
)
pm_data

pip install pymc3>=3.8

prior = pm.sample_prior_predictive()
posterior_predictive = pm.sample_posterior_predictive(trace_g)

pm.data = az.from_pymc3(
    trace=trace_g,
    prior=prior,
    posterior_predictive=posterior_predictive,
)

az.plot_posterior(trace_g)

pm.gelman_rubin(trace_g)

ppc = pm.sample_posterior_predictive(trace_g, samples=1000, model=model_g)
np.asarray(ppc['y']).shape

_, ax = plt.subplots(figsize=(10, 5))
ax.hist([y.mean() for y in ppc['y']], bins=19, alpha=0.5)
ax.axvline(data.EQ.mean())
ax.set(title='Posterior predictive of the mean', xlabel='mean(x)', ylabel='Frequency');

l= len(x_train)
l

x_mod= x_train.drop(["Day"],axis=1)
xt_mod=x_test.drop(["Period"],axis=1)

from sklearn.preprocessing import  PolynomialFeatures
from sklearn.linear_model import  LinearRegression

poly= PolynomialFeatures(degree = 3)
X_poly = poly.fit_transform(x_mod)
poly.fit(X_poly,y_train)
lin= LinearRegression()
lin.fit(X_poly, y_train)

ypred= lin.predict(xt_mod)
error -ypred- y_test
plt.plot(error)

pip install tensorflow

# Let's load the required libs.
# We'll be using the Tensorflow backend (default).
from keras.models import Sequential
from keras.layers.recurrent import LSTM
from keras.layers.core import Dense, Activation, Dropout
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error
from sklearn.utils import shuffle

plt.plot(train_y)

new_data=mean_df[["Day","EQ"]]
new_data.shape

new_test_data=test[["Period","EQ"]]
new__test_data.shape

data_raw = new_data.values.astype("float32")

# We apply the MinMax scaler from sklearn
# to normalize data in the (0, 1) interval.
scaler = MinMaxScaler(feature_range = (0, 1))
dataset = scaler.fit_transform(data_raw)

# Print a few values.
dataset[0:5]

TRAIN_SIZE = 0.80

train_size = int(len(new_data) * TRAIN_SIZE)
test_size = len(new_data) - train_size
train, test = dataset[0:train_size, :], dataset[train_size:len(dataset), :]
print("Number of entries (training set, test set): " + str((len(train), len(test))))

def create_dataset(dataset, window_size = 1):
    data_X, data_Y = [], []
    for i in range(len(dataset) - window_size - 1):
        a = dataset[i:(i + window_size), 0]
        data_X.append(a)
        data_Y.append(dataset[i + window_size, 0])
    return(np.array(data_X), np.array(data_Y))

window_size = 1
train_X, train_Y = create_dataset(train, window_size)
test_X, test_Y = create_dataset(test, window_size)
print("Original training data shape:")
print(train_X.shape)

train_X = np.reshape(train_X, (train_X.shape[0], 1, train_X.shape[1]))
test_X = np.reshape(test_X, (test_X.shape[0], 1, test_X.shape[1]))
print("New training data shape:")
print(train_X.shape)

def fit_model(train_X, train_Y, window_size = 1):
    model = Sequential()
    
    model.add(LSTM(4, 
                   input_shape = (1, window_size)))
    model.add(Dense(1))
    model.compile(loss = "mean_squared_error", 
                  optimizer = "adam")
    model.fit(train_X, 
              train_Y, 
              epochs = 100, 
              batch_size = 10, 
              verbose = 2)
    
    return(model)

# Fit the first model.
model1 = fit_model(train_X, train_Y, window_size)

# Make predictions on the original scale of the data.
    pred = scaler.inverse_transform(model.predict(test_Y))
    # Prepare Y data to also be on the original scale for interpretability.
    orig_data = scaler.inverse_transform([test_Y])
    # Calculate RMSE.
    score = math.sqrt(mean_squared_error(orig_data[0], pred[:, 0]))
    return(score, pred)

rmse_train, train_predict = predict_and_score(model1, train_X, train_Y)
rmse_test, test_predict = predict_and_score(model1, test_X, test_Y)

print("Training data score: %.2f RMSE" % rmse_train)
print("Test data score: %.2f RMSE" % rmse_test)

train_predict_plot = np.empty_like(dataset)
train_predict_plot[:, :] = np.nan
train_predict_plot[window_size:len(train_predict) + window_size, :] = train_predict

# Add test predictions.
test_predict_plot = np.empty_like(dataset)
test_predict_plot[:, :] = np.nan
test_predict_plot[len(train_predict) + (window_size * 2) + 1:len(dataset) - 1, :] = test_predict

# Create the plot.
plt.figure(figsize = (15, 5))
plt.plot(scaler.inverse_transform(dataset), label = "True value")
plt.plot(train_predict_plot, label = "Training set prediction")
plt.plot(test_predict_plot, label = "Test set prediction")
plt.xlabel("Periods")
plt.ylabel("Equity")
plt.title("Comparison true vs. predicted training / test")
plt.legend()
plt.show()

from keras import Model
from keras.layers import  Dense, Input, Dropout

inp = Input(shape=(16,))
hidl = Dense(32, activation='relu')(inp)
hi2dl = Dense(64, activation='sigmoid')(hidl)
hi3dl = Dense(8, activation='relu')(hi2dl)
out = Dense(1,activation= 'linear')(hi3dl)

model = Model(inputs=inp,outputs=out)
model.compile(loss='mse', optimizer= 'adam')

model.fit(features,train_y, shuffle=True, validation_data=[test_set,y_test],epochs=1000, batch_size=90)































